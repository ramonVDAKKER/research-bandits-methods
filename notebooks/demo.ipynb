{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Armed Bandit Demo\n",
    "\n",
    "This notebook demonstrates the usage of various bandit policies implemented in `research-bandits-methods`.\n",
    "\n",
    "We'll simulate a simple K-armed bandit problem and compare different policies:\n",
    "- ε-Greedy\n",
    "- UCB (Upper Confidence Bound)\n",
    "- Gaussian Thompson Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from research_bandits_methods.policies.markovian_policies import (\n",
    "    EpsilonGreedy,\n",
    "    GaussianThompson,\n",
    "    UCB,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Setup: 5-Armed Bandit\n",
    "\n",
    "We'll create a simple bandit environment with 5 arms, each having different mean rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# Bandit setup\n",
    "K = 5  # Number of arms\n",
    "R = 10  # Number of parallel runs for Monte Carlo\n",
    "T = 100  # Number of time steps\n",
    "\n",
    "# True mean rewards for each arm (unknown to the policies)\n",
    "true_means = np.array([0.3, 0.5, 0.8, 0.2, 0.6])\n",
    "optimal_arm = np.argmax(true_means)\n",
    "optimal_reward = true_means[optimal_arm]\n",
    "\n",
    "print(f\"True mean rewards: {true_means}\")\n",
    "print(f\"Optimal arm: {optimal_arm} (mean reward: {optimal_reward:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_bandit(policy, true_means, T, rng):\n",
    "    \"\"\"Simulate a bandit policy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    policy : MarkovianCumPullsCumRewardsPolicy\n",
    "        The policy to simulate.\n",
    "    true_means : np.ndarray\n",
    "        True mean rewards for each arm.\n",
    "    T : int\n",
    "        Number of time steps.\n",
    "    rng : np.random.Generator\n",
    "        Random number generator.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    rewards : np.ndarray\n",
    "        Rewards obtained at each time step, shape (T, R).\n",
    "    arms : np.ndarray\n",
    "        Arms selected at each time step, shape (T, R).\n",
    "    \"\"\"\n",
    "    R = policy.R\n",
    "    rewards = np.zeros((T, R))\n",
    "    arms = np.zeros((T, R), dtype=int)\n",
    "\n",
    "    for t in range(1, T + 1):\n",
    "        # Select arm\n",
    "        arm = policy.select_arm(t)\n",
    "        arms[t - 1] = arm\n",
    "\n",
    "        # Generate rewards (Gaussian with unit variance)\n",
    "        reward = rng.normal(true_means[arm], 1.0)\n",
    "        rewards[t - 1] = reward\n",
    "\n",
    "        # Update policy\n",
    "        policy.update(arm, reward)\n",
    "\n",
    "    return rewards, arms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ε-Greedy Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_greedy = EpsilonGreedy(K=K, R=R, epsilon=0.1, rng=rng)\n",
    "rewards_eg, arms_eg = simulate_bandit(epsilon_greedy, true_means, T, rng)\n",
    "\n",
    "# Calculate cumulative regret (averaged across runs)\n",
    "cumulative_reward_eg = np.cumsum(rewards_eg, axis=0).mean(axis=1)\n",
    "optimal_cumulative = np.arange(1, T + 1) * optimal_reward\n",
    "regret_eg = optimal_cumulative - cumulative_reward_eg\n",
    "\n",
    "print(f\"\\nε-Greedy Policy (ε=0.1):\")\n",
    "print(f\"Final arm pull counts (averaged): {epsilon_greedy.counts.mean(axis=1)}\")\n",
    "print(f\"Estimated means: {epsilon_greedy.means.mean(axis=1)}\")\n",
    "print(f\"Final cumulative regret: {regret_eg[-1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UCB Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucb = UCB(K=K, R=R, c=2.0)\n",
    "rewards_ucb, arms_ucb = simulate_bandit(ucb, true_means, T, rng)\n",
    "\n",
    "cumulative_reward_ucb = np.cumsum(rewards_ucb, axis=0).mean(axis=1)\n",
    "regret_ucb = optimal_cumulative - cumulative_reward_ucb\n",
    "\n",
    "print(f\"\\nUCB Policy (c=2.0):\")\n",
    "print(f\"Final arm pull counts (averaged): {ucb.counts.mean(axis=1)}\")\n",
    "print(f\"Estimated means: {ucb.means.mean(axis=1)}\")\n",
    "print(f\"Final cumulative regret: {regret_ucb[-1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Thompson Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thompson = GaussianThompson(K=K, R=R, prior_mean=0.5, prior_var=1.0, rng=rng)\n",
    "rewards_ts, arms_ts = simulate_bandit(thompson, true_means, T, rng)\n",
    "\n",
    "cumulative_reward_ts = np.cumsum(rewards_ts, axis=0).mean(axis=1)\n",
    "regret_ts = optimal_cumulative - cumulative_reward_ts\n",
    "\n",
    "print(f\"\\nGaussian Thompson Sampling:\")\n",
    "print(f\"Final arm pull counts (averaged): {thompson.counts.mean(axis=1)}\")\n",
    "print(f\"Estimated means: {thompson.means.mean(axis=1)}\")\n",
    "print(f\"Final cumulative regret: {regret_ts[-1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "All three policies successfully learn to identify the optimal arm (arm 2 with mean 0.8) over time.\n",
    "\n",
    "Key observations:\n",
    "- **ε-Greedy**: Simple and effective, but continues to explore uniformly even after finding the best arm\n",
    "- **UCB**: Balances exploration and exploitation using confidence bounds\n",
    "- **Thompson Sampling**: Bayesian approach that naturally balances exploration and exploitation\n",
    "\n",
    "The policies can be compared by examining:\n",
    "1. Final arm pull counts (how many times each arm was selected)\n",
    "2. Estimated means (how well the policy learned the true means)\n",
    "3. Cumulative regret (total opportunity cost from not always selecting the optimal arm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
