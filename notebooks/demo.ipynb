{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Armed Bandit Simulation Framework Demo\n",
    "\n",
    "This notebook demonstrates the research-grade bandit simulation framework with:\n",
    "- Pre-generated counterfactual outcomes\n",
    "- Per-arm variance heterogeneity\n",
    "- Fair multi-policy comparison\n",
    "- Heterogeneous reward distributions per arm\n",
    "- Regret computation and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from research_bandits_methods.bandits import (\n",
    "    BanditEnvironment,\n",
    "    BernoulliRewards,\n",
    "    GaussianRewards,\n",
    "    MultiPolicyComparison,\n",
    "    PerArmDistribution,\n",
    ")\n",
    "from research_bandits_methods.bandits.policies.markovian_policies import (\n",
    "    EpsilonGreedy,\n",
    "    GaussianThompson,\n",
    "    UCB,\n",
    ")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "rng = np.random.default_rng(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Gaussian Rewards with Per-Arm Variances\n",
    "\n",
    "We'll compare policies on a 3-armed bandit where arms have different variances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem parameters\n",
    "T = 500  # Time horizon\n",
    "K = 3  # Number of arms\n",
    "R = 100  # Monte Carlo replications\n",
    "\n",
    "# Gaussian reward distribution with different variances per arm\n",
    "true_means = np.array([0.3, 0.5, 0.8])\n",
    "variances = np.array([1.0, 2.0, 0.5])  # Different variance per arm\n",
    "dist = GaussianRewards(means=true_means, variances=variances)\n",
    "\n",
    "# Create environment (pre-generate all counterfactuals)\n",
    "env = BanditEnvironment(dist, T=T, K=K, R=R, rng=rng)\n",
    "\n",
    "print(f\"Environment created with T={T}, K={K}, R={R}\")\n",
    "print(f\"True means: {true_means}\")\n",
    "print(f\"Variances: {variances}\")\n",
    "print(f\"Optimal arm: {true_means.argmax()} (mean: {true_means.max():.2f})\")\n",
    "print(f\"Counterfactuals shape: {env.counterfactuals.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Multi-Policy Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison object\n",
    "comparison = MultiPolicyComparison(env)\n",
    "\n",
    "# Add policies to compare\n",
    "comparison.add_policy(\"ε-greedy (ε=0.1)\", EpsilonGreedy(K=K, R=R, epsilon=0.1, rng=rng))\n",
    "comparison.add_policy(\n",
    "    \"ε-greedy (ε=0.05)\", EpsilonGreedy(K=K, R=R, epsilon=0.05, rng=rng)\n",
    ")\n",
    "comparison.add_policy(\"UCB (c=1.0)\", UCB(K=K, R=R, c=1.0))\n",
    "comparison.add_policy(\"UCB (c=2.0)\", UCB(K=K, R=R, c=2.0))\n",
    "comparison.add_policy(\n",
    "    \"Thompson Sampling\",\n",
    "    GaussianThompson(K=K, R=R, prior_mean=0.5, prior_var=1.0, rng=rng),\n",
    ")\n",
    "\n",
    "# Run all policies on the same counterfactual data\n",
    "comparison.run_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary\n",
    "comparison.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regret Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get regret curves with confidence intervals\n",
    "regret_curves = comparison.get_regret_curves_with_ci(confidence=0.95)\n",
    "\n",
    "print(\"\\nFinal average cumulative regret (with 95% CI):\\n\")\n",
    "for policy_name, curve_data in regret_curves.items():\n",
    "    final_mean = curve_data[\"mean\"][-1]\n",
    "    final_lower = curve_data[\"lower\"][-1]\n",
    "    final_upper = curve_data[\"upper\"][-1]\n",
    "    print(\n",
    "        f\"{policy_name:25s}: {final_mean:7.2f} [{final_lower:7.2f}, {final_upper:7.2f}]\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Heterogeneous Distributions Per Arm\n",
    "\n",
    "Different arms can have completely different reward distributions:\n",
    "- Arm 0: Bernoulli(0.3)\n",
    "- Arm 1: Gaussian(0.5, 1.0)\n",
    "- Arm 2: Gaussian(0.8, 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define heterogeneous distributions\n",
    "arm_distributions = [\n",
    "    BernoulliRewards([0.3]),\n",
    "    GaussianRewards([0.5], variances=1.0),\n",
    "    GaussianRewards([0.8], variances=2.0),\n",
    "]\n",
    "\n",
    "# Create per-arm distribution\n",
    "heterogeneous_dist = PerArmDistribution(arm_distributions)\n",
    "\n",
    "# Create environment\n",
    "env2 = BanditEnvironment(heterogeneous_dist, T=500, K=3, R=50, rng=rng)\n",
    "\n",
    "print(\"Heterogeneous environment created\")\n",
    "print(\"Arm 0: Bernoulli(0.3)\")\n",
    "print(\"Arm 1: Gaussian(0.5, 1.0)\")\n",
    "print(\"Arm 2: Gaussian(0.8, 2.0) - high variance!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Comparison on Heterogeneous Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison2 = MultiPolicyComparison(env2)\n",
    "\n",
    "comparison2.add_policy(\"UCB\", UCB(K=3, R=50, c=2.0))\n",
    "comparison2.add_policy(\"ε-greedy\", EpsilonGreedy(K=3, R=50, epsilon=0.1, rng=rng))\n",
    "comparison2.add_policy(\n",
    "    \"Thompson\", GaussianThompson(K=3, R=50, prior_mean=0.5, prior_var=1.0, rng=rng)\n",
    ")\n",
    "\n",
    "comparison2.run_all()\n",
    "comparison2.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Analyzing Arm Selection Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get detailed results from first comparison\n",
    "summary = comparison.get_results_summary()\n",
    "\n",
    "print(\"\\nArm Pull Counts (average across replications):\\n\")\n",
    "print(f\"{'Policy':<25s} {'Arm 0':>10s} {'Arm 1':>10s} {'Arm 2':>10s}\")\n",
    "print(\"-\" * 60)\n",
    "for policy_name, stats in summary.items():\n",
    "    counts = stats[\"arm_pull_counts\"]\n",
    "    print(f\"{policy_name:<25s} {counts[0]:10.1f} {counts[1]:10.1f} {counts[2]:10.1f}\")\n",
    "\n",
    "print(\n",
    "    f\"\\nNote: Optimal arm is 2 (mean {true_means[2]:.2f}, variance {variances[2]:.2f})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This framework provides:\n",
    "\n",
    "1. **Fair Comparison**: All policies run on identical counterfactual outcomes\n",
    "2. **Per-Arm Heterogeneity**: Different means and variances per arm\n",
    "3. **Flexible Distributions**: Mix Bernoulli, Gaussian, Student's t, etc.\n",
    "4. **Efficiency**: Pre-generated counterfactuals enable fast experimentation\n",
    "5. **Extensibility**: Easy to add new distributions and policies\n",
    "\n",
    "### Key Observations\n",
    "\n",
    "- **UCB** typically has theoretical guarantees but may be conservative\n",
    "- **ε-greedy** is simple but continues exploring uniformly\n",
    "- **Thompson Sampling** balances exploration/exploitation naturally via Bayesian approach\n",
    "- **Per-arm variances** allow realistic modeling of heterogeneous reward uncertainty\n",
    "- **Heterogeneous distributions** enable arms with fundamentally different reward structures"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
